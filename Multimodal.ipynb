{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdec48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üîß Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MULTIMODAL RAG SYSTEM - STEP BY STEP JUPYTER NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Environment Setup and Imports\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Think of this step as preparing your workspace - like setting up all the tools \n",
    "you need before starting a complex project. We're importing libraries that will\n",
    "help us read PDFs, process images, work with AI models, and create our smart\n",
    "document analysis system.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "We're setting up the environment with all necessary dependencies for:\n",
    "- PDF processing (unstructured)\n",
    "- AI/LLM integration (langchain, openai)\n",
    "- Image processing (pytesseract)\n",
    "- Vector storage and retrieval (chroma, embeddings)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables (like API keys)\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Import LangChain components for AI model integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Import document processing libraries\n",
    "from typing import Any\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import base64\n",
    "import uuid\n",
    "\n",
    "# Import vector database components\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üîß Environment setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae48c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI Models initialized:\n",
      "   - GPT-3.5 Turbo for text processing\n",
      "   - GPT-4 Vision for image analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Initialize AI Models\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Here we're setting up our AI \"assistants\" - think of them as different specialists:\n",
    "- One for reading and summarizing text (GPT-3.5)\n",
    "- One for analyzing images (GPT-4 Vision)\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "We initialize two different OpenAI models:\n",
    "- GPT-3.5 Turbo: Fast and efficient for text processing\n",
    "- GPT-4 Vision: Advanced model capable of understanding images\n",
    "\"\"\"\n",
    "\n",
    "# Initialize AI models\n",
    "chain_gpt_35 = ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=1024)\n",
    "chain_gpt_4_vision = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "print(\"ü§ñ AI Models initialized:\")\n",
    "print(\"   - GPT-3.5 Turbo for text processing\")\n",
    "print(\"   - GPT-4 Vision for image analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2e4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëÅÔ∏è OCR configured for image text extraction\n",
      "üìÅ Input path: d:\\Multimodel_LLMs\\GenerativeAI_Multimodal_LLMs\n",
      "üñºÔ∏è Image output path: d:\\Multimodel_LLMs\\GenerativeAI_Multimodal_LLMs\\figures\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Configure Tesseract OCR (if needed)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Tesseract is like having a digital eye that can read text from images.\n",
    "If your PDF has text embedded as images, this tool helps extract it.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Configure Tesseract OCR for extracting text from images within PDFs.\n",
    "Adjust the path based on your system installation.\n",
    "\"\"\"\n",
    "\n",
    "# Configure Tesseract path (adjust for your system)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Set up paths\n",
    "input_path = os.getcwd()\n",
    "output_path = os.path.join(os.getcwd(), \"figures\")\n",
    "\n",
    "print(\"üëÅÔ∏è OCR configured for image text extraction\")\n",
    "print(f\"üìÅ Input path: {input_path}\")\n",
    "print(f\"üñºÔ∏è Image output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91abd008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Starting PDF processing...\n",
      "‚öôÔ∏è Using enhanced table detection settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF processed successfully!\n",
      "üìä Found 1 total elements in the PDF\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: PDF Processing and Element Extraction\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "This is where the magic begins! We're taking your PDF and breaking it down into\n",
    "different types of content - like sorting a mixed pile of documents into \n",
    "separate stacks of text pages, data tables, and pictures.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Using the unstructured library to parse the PDF with enhanced settings:\n",
    "- Extract images and infer table structures\n",
    "- Use high-resolution processing for better accuracy\n",
    "- Chunk content intelligently for better processing\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Starting PDF processing...\")\n",
    "print(\"‚öôÔ∏è Using enhanced table detection settings\")\n",
    "\n",
    "# Enhanced PDF processing with better table detection\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(input_path, \"startupai-financial-report-v2.pdf\"),\n",
    "    extract_images_in_pdf=True,      # Extract all images from PDF\n",
    "    infer_table_structure=True,      # Try to detect table structures\n",
    "    chunking_strategy=\"by_title\",    # Group content by document sections\n",
    "    max_characters=4000,             # Maximum chunk size\n",
    "    new_after_n_chars=3800,         # When to create new chunks\n",
    "    combine_text_under_n_chars=2000, # Combine small text pieces\n",
    "    image_output_dir_path=output_path, # Where to save extracted images\n",
    "    strategy=\"hi_res\",               # High resolution processing\n",
    "    hi_res_model_name=\"yolox\",      # Advanced table detection model\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ PDF processed successfully!\")\n",
    "print(f\"üìä Found {len(raw_pdf_elements)} total elements in the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0a05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing and classifying document elements...\n",
      "==================================================\n",
      "Element 1:\n",
      "  Type: <class 'unstructured.documents.elements.CompositeElement'>\n",
      "  Content preview: >\n",
      "\n",
      "FINANCIAL\n",
      "\n",
      "STATEMENT\n",
      "\n",
      "Explore our financial performance through balance sheets, income, and cash ...\n",
      "  ‚ûú Classified as: TEXT\n",
      "------------------------------\n",
      "\n",
      "üìã Initial Classification Results:\n",
      "   üìù Text elements: 1\n",
      "   üìä Table elements: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Element Classification and Analysis\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Now we're like a librarian sorting through all the content we extracted.\n",
    "We're putting each piece into the right category: text, tables, or images.\n",
    "If the automatic sorting misses something important (like financial tables),\n",
    "we have a smart backup system that looks for financial keywords.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Classify extracted elements by type and implement fallback logic for\n",
    "table detection. This step is crucial because PDF table detection can\n",
    "be unreliable, so we use content-based classification as backup.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize storage lists for different content types\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "print(\"\\nüîç Analyzing and classifying document elements...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze each element and classify it\n",
    "for i, element in enumerate(raw_pdf_elements):\n",
    "    element_type = str(type(element))\n",
    "    content_preview = str(element)[:100]\n",
    "    \n",
    "    print(f\"Element {i+1}:\")\n",
    "    print(f\"  Type: {element_type}\")\n",
    "    print(f\"  Content preview: {content_preview}...\")\n",
    "    \n",
    "    # Classify elements based on their type\n",
    "    if 'CompositeElement' in element_type:\n",
    "        text_elements.append(element)\n",
    "        print(f\"  ‚ûú Classified as: TEXT\")\n",
    "    elif 'Table' in element_type:\n",
    "        table_elements.append(element)\n",
    "        print(f\"  ‚ûú Classified as: TABLE\")\n",
    "    elif 'FigureCaption' in element_type:\n",
    "        text_elements.append(element)\n",
    "        print(f\"  ‚ûú Classified as: TEXT (Figure Caption)\")\n",
    "    else:\n",
    "        # Smart classification: check if content contains financial keywords\n",
    "        if any(keyword in content_preview.lower() for keyword in \n",
    "               ['gross income', 'total expenses', 'net income', 'taxes', 'revenue']):\n",
    "            table_elements.append(element)\n",
    "            print(f\"  ‚ûú Classified as: TABLE (Financial Content Detected)\")\n",
    "        else:\n",
    "            text_elements.append(element)\n",
    "            print(f\"  ‚ûú Classified as: TEXT (Default)\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Convert elements to text format\n",
    "table_elements = [i.text if hasattr(i, 'text') else str(i) for i in table_elements]\n",
    "text_elements = [i.text if hasattr(i, 'text') else str(i) for i in text_elements]\n",
    "\n",
    "print(f\"\\nüìã Initial Classification Results:\")\n",
    "print(f\"   üìù Text elements: {len(text_elements)}\")\n",
    "print(f\"   üìä Table elements: {len(table_elements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0166a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No tables detected automatically\n",
      "üîß Attempting manual extraction using financial keywords...\n",
      "‚úÖ Successfully extracted 1 financial table(s) manually\n",
      "\n",
      "üìä Final Element Count:\n",
      "   üìù Text elements: 1\n",
      "   üìä Table elements: 1\n",
      "\n",
      "üí∞ Financial Table Content Found:\n",
      "Table 1 preview:\n",
      "   >\n",
      "\n",
      "FINANCIAL\n",
      "\n",
      "STATEMENT\n",
      "\n",
      "Explore our financial performance through balance sheets, income, and cash flow statements.\n",
      "\n",
      "DELAITTE\n",
      "\n",
      "StartupAI boasts an impressive return on investment (ROI), demonstrating...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Manual Table Extraction (Fallback System)\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Sometimes the automatic table detector doesn't work perfectly (like having\n",
    "trouble reading someone's handwriting). So we have a backup system that\n",
    "specifically looks for financial terms and treats that content as table data.\n",
    "This ensures we don't miss important financial information.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Implement a fallback mechanism for table detection using keyword-based\n",
    "content analysis. This addresses the common issue where PDFs store tables\n",
    "as formatted text rather than true table structures.\n",
    "\"\"\"\n",
    "\n",
    "def extract_financial_data_manually(text_content):\n",
    "    \"\"\"\n",
    "    Fallback function to extract financial table data when automatic detection fails\n",
    "    \n",
    "    Args:\n",
    "        text_content: List of text elements to search through\n",
    "    \n",
    "    Returns:\n",
    "        List of text segments that contain financial data\n",
    "    \"\"\"\n",
    "    financial_tables = []\n",
    "    \n",
    "    # Keywords that indicate financial/table data\n",
    "    financial_keywords = [\n",
    "        'gross income', 'total expenses', 'net income', 'taxes', \n",
    "        'revenue', 'profit', 'loss', 'balance', 'assets', 'roi'\n",
    "    ]\n",
    "    \n",
    "    for text in text_content:\n",
    "        # Check if text contains multiple financial keywords (likely a table)\n",
    "        keyword_count = sum(1 for keyword in financial_keywords \n",
    "                          if keyword in text.lower())\n",
    "        \n",
    "        if keyword_count >= 2:  # If it has 2+ financial terms, treat as table\n",
    "            financial_tables.append(text)\n",
    "    \n",
    "    return financial_tables\n",
    "\n",
    "# Apply manual extraction if needed\n",
    "if len(table_elements) == 0:\n",
    "    print(\"‚ö†Ô∏è No tables detected automatically\")\n",
    "    print(\"üîß Attempting manual extraction using financial keywords...\")\n",
    "    \n",
    "    manual_tables = extract_financial_data_manually(text_elements)\n",
    "    \n",
    "    if manual_tables:\n",
    "        table_elements.extend(manual_tables)\n",
    "        print(f\"‚úÖ Successfully extracted {len(manual_tables)} financial table(s) manually\")\n",
    "    else:\n",
    "        print(\"‚ùå No financial data patterns found\")\n",
    "else:\n",
    "    print(\"‚úÖ Tables detected automatically\")\n",
    "\n",
    "print(f\"\\nüìä Final Element Count:\")\n",
    "print(f\"   üìù Text elements: {len(text_elements)}\")\n",
    "print(f\"   üìä Table elements: {len(table_elements)}\")\n",
    "\n",
    "# Show extracted table content\n",
    "if table_elements:\n",
    "    print(f\"\\nüí∞ Financial Table Content Found:\")\n",
    "    for i, table in enumerate(table_elements):\n",
    "        print(f\"Table {i+1} preview:\")\n",
    "        print(f\"   {table[:200]}...\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08233816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Processing extracted images...\n",
      "   üì∏ Processed image: figure-1-1.jpg\n",
      "   üì∏ Processed image: figure-1-2.jpg\n",
      "   üì∏ Processed image: figure-1-3.jpg\n",
      "   üì∏ Processed image: figure-1-4.jpg\n",
      "   üì∏ Processed image: figure-1-5.jpg\n",
      "   üì∏ Processed image: figure-1-6.jpg\n",
      "\n",
      "üñºÔ∏è Final Image Count: 6 images processed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Image Processing\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Now we're processing all the images that were extracted from the PDF.\n",
    "We convert them into a format that the AI can understand, and count how\n",
    "many images we have to work with.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Process extracted images by encoding them in base64 format for AI analysis.\n",
    "Base64 encoding converts images into text strings that can be sent to\n",
    "vision-capable AI models.\n",
    "\"\"\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Convert image file to base64 encoding for AI processing\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string representation of the image\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(\"üñºÔ∏è Processing extracted images...\")\n",
    "\n",
    "# Process all extracted images\n",
    "image_count = 0\n",
    "for image_file in os.listdir(output_path):\n",
    "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(output_path, image_file)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "        image_count += 1\n",
    "        print(f\"   üì∏ Processed image: {image_file}\")\n",
    "\n",
    "print(f\"\\nüñºÔ∏è Final Image Count: {len(image_elements)} images processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89da61ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Starting AI-powered content analysis...\n",
      "==================================================\n",
      "üìù Processing 1 text elements with GPT-3.5...\n",
      "   Processing text element 1...\n",
      "   ‚úÖ Text element 1 summarized\n",
      "   Summary preview: StartupAI has shown impressive financial performance with a $22 million sales figure, $15 million in...\n",
      "\n",
      "üìä Processing 1 table elements with GPT-3.5...\n",
      "   Processing table element 1...\n",
      "   ‚úÖ Table element 1 analyzed\n",
      "   Summary preview: 1. This financial information contains data on the gross income, total expenses, taxes, and net income of StartupAI.\n",
      "2. The key figures and amounts ar...\n",
      "\n",
      "üñºÔ∏è Processing 6 images with GPT-4 Vision...\n",
      "   Analyzing image 1...\n",
      "   ‚úÖ Image 1 analyzed\n",
      "   Description preview: The image is a geometric logo design consisting of two interlocking shapes. The left shape is yellow...\n",
      "\n",
      "   Analyzing image 2...\n",
      "   ‚úÖ Image 2 analyzed\n",
      "   Description preview: The image features the words \"FINANCIAL STATEMENT\" in bold, capital letters. The word \"FINANCIAL\" is...\n",
      "\n",
      "   Analyzing image 3...\n",
      "   ‚úÖ Image 3 analyzed\n",
      "   Description preview: The image features the word \"STATEMENT\" in bold, white capital letters against a solid dark blue bac...\n",
      "\n",
      "   Analyzing image 4...\n",
      "   ‚úÖ Image 4 analyzed\n",
      "   Description preview: The image shows a graphics card, which is a computer hardware component used to render images and vi...\n",
      "\n",
      "   Analyzing image 5...\n",
      "   ‚úÖ Image 5 analyzed\n",
      "   Description preview: The image contains the text \"33% ROI\" in a bold, dark blue font. The text is centered on a white bac...\n",
      "\n",
      "   Analyzing image 6...\n",
      "   ‚úÖ Image 6 analyzed\n",
      "   Description preview: The image is a bar chart with vertical columns representing data from the years 2020 to 2024. The ba...\n",
      "\n",
      "üéâ Content summarization complete!\n",
      "üìã Summary Statistics:\n",
      "   üìù Text summaries: 1\n",
      "   üìä Table summaries: 1\n",
      "   üñºÔ∏è Image summaries: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: AI-Powered Content Summarization\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Now comes the really cool part! We're sending each piece of content to our\n",
    "AI specialists to create smart summaries. It's like having three different\n",
    "experts: one for reading text, one for analyzing tables, and one for \n",
    "describing images. Each expert creates a summary that captures the key information.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Use different AI models to create summaries of each content type:\n",
    "- GPT-3.5 for text and table summarization\n",
    "- GPT-4 Vision for image analysis\n",
    "These summaries will be stored in our vector database for retrieval.\n",
    "\"\"\"\n",
    "\n",
    "# Define summarization functions for each content type\n",
    "\n",
    "def summarize_text(text_element):\n",
    "    \"\"\"Summarize text content using GPT-3.5\"\"\"\n",
    "    prompt = f\"Summarize the following text concisely:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def summarize_table(table_element):\n",
    "    \"\"\"Analyze and summarize table/financial data using GPT-3.5\"\"\"\n",
    "    prompt = f\"\"\"Analyze the following financial table/data:\n",
    "\n",
    "{table_element}\n",
    "\n",
    "Provide a clear summary that includes:\n",
    "1. What type of financial information this contains\n",
    "2. Key figures and amounts\n",
    "3. Any important financial metrics or ratios\n",
    "\n",
    "Summary:\"\"\"\n",
    "    response = chain_gpt_35.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "def summarize_image(encoded_image):\n",
    "    \"\"\"Analyze and describe image content using GPT-4 Vision\"\"\"\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are an expert at analyzing images and charts.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this image in detail. If it contains financial data, tables, charts, or business information, provide specific details about numbers, metrics, and visual elements.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = chain_gpt_4_vision.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Process each content type\n",
    "print(\"üß† Starting AI-powered content analysis...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process text elements\n",
    "text_summaries = []\n",
    "if text_elements:\n",
    "    print(f\"üìù Processing {len(text_elements)} text elements with GPT-3.5...\")\n",
    "    for i, te in enumerate(text_elements[:5]):  # Limit to first 5 for demo\n",
    "        print(f\"   Processing text element {i + 1}...\")\n",
    "        summary = summarize_text(te)\n",
    "        text_summaries.append(summary)\n",
    "        print(f\"   ‚úÖ Text element {i + 1} summarized\")\n",
    "        print(f\"   Summary preview: {summary[:100]}...\")\n",
    "        print()\n",
    "\n",
    "# Process table elements\n",
    "table_summaries = []\n",
    "if table_elements:\n",
    "    print(f\"üìä Processing {len(table_elements)} table elements with GPT-3.5...\")\n",
    "    for i, te in enumerate(table_elements):\n",
    "        print(f\"   Processing table element {i + 1}...\")\n",
    "        summary = summarize_table(te)\n",
    "        table_summaries.append(summary)\n",
    "        print(f\"   ‚úÖ Table element {i + 1} analyzed\")\n",
    "        print(f\"   Summary preview: {summary[:150]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No table elements to process\")\n",
    "\n",
    "# Process image elements\n",
    "image_summaries = []\n",
    "if image_elements:\n",
    "    print(f\"üñºÔ∏è Processing {len(image_elements)} images with GPT-4 Vision...\")\n",
    "    for i, ie in enumerate(image_elements[:8]):  # Limit to first 8 for demo\n",
    "        print(f\"   Analyzing image {i + 1}...\")\n",
    "        summary = summarize_image(ie)\n",
    "        image_summaries.append(summary)\n",
    "        print(f\"   ‚úÖ Image {i + 1} analyzed\")\n",
    "        print(f\"   Description preview: {summary[:100]}...\")\n",
    "        print()\n",
    "\n",
    "print(\"üéâ Content summarization complete!\")\n",
    "print(f\"üìã Summary Statistics:\")\n",
    "print(f\"   üìù Text summaries: {len(text_summaries)}\")\n",
    "print(f\"   üìä Table summaries: {len(table_summaries)}\")\n",
    "print(f\"   üñºÔ∏è Image summaries: {len(image_summaries)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c290fe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÉÔ∏è Setting up the intelligent vector database...\n",
      "‚úÖ Vector database initialized\n",
      "   üßÆ Embedding function: OpenAI Embeddings\n",
      "   üíæ Vector store: ChromaDB\n",
      "   üîó Retriever: MultiVector (summaries + original content)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: Vector Database Setup\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Now we're setting up our smart filing system! Think of it like creating a\n",
    "magical library where instead of organizing books by alphabet, we organize\n",
    "them by meaning and context. When you ask a question, the system can quickly\n",
    "find the most relevant information, even if the exact words don't match.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Initialize the vector database components:\n",
    "- ChromaDB for storing vector embeddings\n",
    "- OpenAI embeddings for converting text to vectors\n",
    "- MultiVectorRetriever for managing the relationship between summaries and original content\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÉÔ∏è Setting up the intelligent vector database...\")\n",
    "\n",
    "# Initialize vector database components\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multimodal_summaries\", \n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "docstore = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=docstore, \n",
    "    id_key=id_key\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector database initialized\")\n",
    "print(\"   üßÆ Embedding function: OpenAI Embeddings\")\n",
    "print(\"   üíæ Vector store: ChromaDB\")\n",
    "print(\"   üîó Retriever: MultiVector (summaries + original content)\")\n",
    "\n",
    "def add_documents_to_retriever(summaries, original_contents, content_type):\n",
    "    \"\"\"\n",
    "    Add documents to the vector database with metadata\n",
    "    \n",
    "    Args:\n",
    "        summaries: List of AI-generated summaries\n",
    "        original_contents: List of original content\n",
    "        content_type: Type of content (text, table, image)\n",
    "    \"\"\"\n",
    "    if not summaries:\n",
    "        print(f\"‚ö†Ô∏è No {content_type} summaries to add - skipping\")\n",
    "        return\n",
    "    \n",
    "    # Generate unique IDs for each document\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    \n",
    "    # Create document objects with metadata\n",
    "    summary_docs = [\n",
    "        Document(\n",
    "            page_content=summary, \n",
    "            metadata={\n",
    "                id_key: doc_ids[i], \n",
    "                \"content_type\": content_type,\n",
    "                \"source\": \"financial_report\"\n",
    "            }\n",
    "        )\n",
    "        for i, summary in enumerate(summaries)\n",
    "    ]\n",
    "    \n",
    "    # Add to vector database\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    \n",
    "    # Store original content for retrieval\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))\n",
    "    \n",
    "    print(f\"‚úÖ Added {len(summaries)} {content_type} documents to vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93bbfb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Populating vector database with processed content...\n",
      "==================================================\n",
      "‚úÖ Added 1 text documents to vector database\n",
      "‚úÖ Added 1 table documents to vector database\n",
      "‚úÖ Added 6 image documents to vector database\n",
      "\n",
      "üéâ Vector database population complete!\n",
      "üîç The system is now ready for intelligent querying\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: Populate the Vector Database\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "We're now putting all our organized content into the smart filing system.\n",
    "Each summary gets stored with a special \"fingerprint\" (vector) that represents\n",
    "its meaning. This allows the system to find relevant information based on\n",
    "the meaning of your questions, not just keyword matching.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Add all summarized content to the vector database. Each summary is converted\n",
    "to embeddings (vector representations) that enable semantic search capabilities.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìö Populating vector database with processed content...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Add text content\n",
    "if text_summaries:\n",
    "    add_documents_to_retriever(\n",
    "        text_summaries, \n",
    "        text_elements[:len(text_summaries)], \n",
    "        \"text\"\n",
    "    )\n",
    "\n",
    "# Add table content\n",
    "if table_summaries:\n",
    "    add_documents_to_retriever(\n",
    "        table_summaries, \n",
    "        table_elements, \n",
    "        \"table\"\n",
    "    )\n",
    "\n",
    "# Add image content\n",
    "if image_summaries:\n",
    "    add_documents_to_retriever(\n",
    "        image_summaries, \n",
    "        image_summaries,  # For images, we store summaries as original content\n",
    "        \"image\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüéâ Vector database population complete!\")\n",
    "print(\"üîç The system is now ready for intelligent querying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14b0a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Setting up the intelligent question-answering system...\n",
      "‚úÖ Question-answering system ready!\n",
      "   üîó RAG Chain: Retriever ‚Üí Prompt ‚Üí LLM ‚Üí Answer\n",
      "   ü§ñ Answer Model: GPT-3.5 Turbo\n",
      "   üéØ Temperature: 0 (for consistent, factual answers)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: Question-Answering System Setup\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Now we're creating the \"brain\" of our system - the part that can answer your\n",
    "questions! When you ask something, it will search through all the content\n",
    "(text, tables, images) to find relevant information, then use AI to generate\n",
    "a comprehensive answer based on what it found.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Set up the RAG (Retrieval-Augmented Generation) pipeline:\n",
    "- Retriever finds relevant content based on semantic similarity\n",
    "- Prompt template formats the context and question\n",
    "- LLM generates answers based on retrieved context\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß† Setting up the intelligent question-answering system...\")\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"You are an intelligent financial document analyst. Answer the question based ONLY on the following context, which includes information from text, tables, and images from the financial report.\n",
    "\n",
    "Context from the document:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide specific, accurate answers based only on the context provided\n",
    "- Include exact numbers and figures when available\n",
    "- If the context doesn't contain enough information, say so clearly\n",
    "- For financial data, be precise with amounts and percentages\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the language model for answering\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create the complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Question-answering system ready!\")\n",
    "print(\"   üîó RAG Chain: Retriever ‚Üí Prompt ‚Üí LLM ‚Üí Answer\")\n",
    "print(\"   ü§ñ Answer Model: GPT-3.5 Turbo\")\n",
    "print(\"   üéØ Temperature: 0 (for consistent, factual answers)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e9e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ TESTING THE MULTIMODAL RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "‚ùì Question: What is the company's gross income?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The company's gross income is $22,000,000.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What are the total expenses?\n",
      "--------------------------------------------------\n",
      "üí° Answer: Total expenses are $2,000,000.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What is the net income?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The net income is $15,000,000.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: How much did the company pay in taxes?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The company paid $5,000,000 in taxes.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What is the ROI percentage?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The ROI percentage is 33%.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What product does the company sell?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The company sells a product or service related to artificial intelligence (AI) based on the name \"StartupAI\" and its website www.startupAI.com.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: Give me a complete financial summary of the company\n",
      "--------------------------------------------------\n",
      "üí° Answer: Based on the information provided in the financial document, the financial summary of the company is as follows:\n",
      "\n",
      "- Gross Income: $22,000,000\n",
      "- Total Expenses: $2,000,000\n",
      "- Taxes: $5,000,000\n",
      "- Net Income: $15,000,000\n",
      "- Return on Investment (ROI): 33%\n",
      "- Sales: $22,000,000\n",
      "\n",
      "This summary highlights the company's strong financial performance, with significant sales, a high ROI, and a substantial net income after expenses and taxes.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What financial data is available in the document?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The financial data available in the document includes:\n",
      "- Gross Income: $22,000,000\n",
      "- Total Expenses: $2,000,000\n",
      "- Taxes: $5,000,000\n",
      "- Net Income: $15,000,000\n",
      "- Return on Investment (ROI): 33%\n",
      "- Sales: $22,000,000\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What is the name of the company?\n",
      "--------------------------------------------------\n",
      "üí° Answer: The name of the company is StartupAI.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What type of business is this company in?\n",
      "--------------------------------------------------\n",
      "üí° Answer: Based on the information provided, this company is in the technology or artificial intelligence (AI) business. This is indicated by the company name \"StartupAI\" and the mention of its market dominance, strong customer appeal, and impressive financial performance, including a gross income of $22,000,000 and a return on investment (ROI) of 33%.\n",
      "--------------------------------------------------\n",
      "\n",
      "üéâ Testing complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: Test the System with Sample Questions\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Time for the exciting part - testing our smart system! We'll ask it various\n",
    "questions about the financial report to see how well it can find and combine\n",
    "information from different sources (text, tables, images) to give accurate answers.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Test the complete multimodal RAG system with a variety of questions that\n",
    "require different types of information retrieval and synthesis.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ TESTING THE MULTIMODAL RAG SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comprehensive test questions\n",
    "test_questions = [\n",
    "    # Financial data questions (should use table information)\n",
    "    \"What is the company's gross income?\",\n",
    "    \"What are the total expenses?\",\n",
    "    \"What is the net income?\",\n",
    "    \"How much did the company pay in taxes?\",\n",
    "    \n",
    "    # Visual/image-based questions\n",
    "    \"What is the ROI percentage?\",\n",
    "    \"What product does the company sell?\",\n",
    "    \n",
    "    # Comprehensive questions (require multiple sources)\n",
    "    \"Give me a complete financial summary of the company\",\n",
    "    \"What financial data is available in the document?\",\n",
    "    \n",
    "    # Company information questions\n",
    "    \"What is the name of the company?\",\n",
    "    \"What type of business is this company in?\",\n",
    "]\n",
    "\n",
    "def test_question(question, show_context=False):\n",
    "    \"\"\"\n",
    "    Test a single question and optionally show retrieved context\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        show_context: Whether to show the retrieved context\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get the answer\n",
    "        answer = rag_chain.invoke(question)\n",
    "        \n",
    "        # Optionally show retrieved context\n",
    "        if show_context:\n",
    "            context = retriever.invoke(question)\n",
    "            print(f\"üìã Retrieved Context:\")\n",
    "            for i, doc in enumerate(context):\n",
    "                print(f\"   {i+1}. {doc[:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"üí° Answer: {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test each question\n",
    "for question in test_questions:\n",
    "    test_question(question, show_context=False)\n",
    "\n",
    "print(\"\\nüéâ Testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330bbae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù To use the interactive interface, uncomment and run:\n",
      "ask_custom_question()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: Interactive Question Interface\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "This creates an interactive interface where you can ask your own questions\n",
    "about the financial document. Just run this cell and ask anything you want\n",
    "to know about the company's finances, products, or performance!\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Provide an interactive interface for custom queries. This allows users to\n",
    "explore the document interactively and test the system's capabilities\n",
    "with their own questions.\n",
    "\"\"\"\n",
    "\n",
    "def ask_custom_question():\n",
    "    \"\"\"\n",
    "    Interactive function to ask custom questions about the document\n",
    "    \"\"\"\n",
    "    print(\"\\nü§î Interactive Question Interface\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Ask me anything about the financial document!\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Your question: \")\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for using the multimodal RAG system!\")\n",
    "            break\n",
    "        \n",
    "        if question.strip():\n",
    "            test_question(question, show_context=True)\n",
    "        else:\n",
    "            print(\"Please enter a valid question.\")\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# ask_custom_question()\n",
    "\n",
    "print(\"\\nüìù To use the interactive interface, uncomment and run:\")\n",
    "print(\"ask_custom_question()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624c72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SYSTEM PERFORMANCE ANALYSIS\n",
      "==================================================\n",
      "üìã Document Processing Summary:\n",
      "   üìÑ PDF Elements Processed: 1\n",
      "   üìù Text Elements: 1\n",
      "   üìä Table Elements: 1\n",
      "   üñºÔ∏è Image Elements: 6\n",
      "\n",
      "üß† AI Analysis Summary:\n",
      "   üìù Text Summaries Generated: 1\n",
      "   üìä Table Summaries Generated: 1\n",
      "   üñºÔ∏è Image Summaries Generated: 6\n",
      "\n",
      "üéØ System Capabilities:\n",
      "   üóÉÔ∏è Total Content Pieces Indexed: 8\n",
      "   üîç Multimodal Search: ‚úÖ Enabled\n",
      "   ü§ñ AI-Powered Summarization: ‚úÖ Active\n",
      "   üíæ Vector Database: ‚úÖ Populated\n",
      "   ‚ùì Question Answering: ‚úÖ Ready\n",
      "\n",
      "üåü Key Achievements:\n",
      "   ‚úÖ Successfully extracted financial data from complex PDF\n",
      "   ‚úÖ Processed multiple content types (text, tables, images)\n",
      "   ‚úÖ Created intelligent summaries of all content\n",
      "   ‚úÖ Built searchable knowledge base\n",
      "   ‚úÖ Enabled natural language querying\n",
      "\n",
      "üí° What makes this system special:\n",
      "   üéØ Multimodal: Understands text, tables, AND images\n",
      "   üß† Semantic Search: Finds relevant info by meaning, not just keywords\n",
      "   üìä Financial Intelligence: Specialized for business document analysis\n",
      "   üîÑ Adaptive: Falls back to manual extraction when needed\n",
      "   üé™ End-to-End: Complete pipeline from PDF to answers\n",
      "\n",
      "üéâ Your multimodal RAG system is fully operational!\n",
      "üöÄ Ready for real-world financial document analysis!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 14: System Performance Analysis\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LAYMAN EXPLANATION:\n",
    "Let's analyze how well our system performed! We'll look at what types of\n",
    "content it found, how accurately it answered questions, and what makes\n",
    "this system special compared to a regular AI chatbot.\n",
    "\n",
    "TECHNICAL EXPLANATION:\n",
    "Analyze the system's performance, content processing statistics, and\n",
    "demonstrate the value of the multimodal RAG approach.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä SYSTEM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Content processing statistics\n",
    "print(f\"üìã Document Processing Summary:\")\n",
    "print(f\"   üìÑ PDF Elements Processed: {len(raw_pdf_elements)}\")\n",
    "print(f\"   üìù Text Elements: {len(text_elements)}\")\n",
    "print(f\"   üìä Table Elements: {len(table_elements)}\")\n",
    "print(f\"   üñºÔ∏è Image Elements: {len(image_elements)}\")\n",
    "print()\n",
    "\n",
    "print(f\"üß† AI Analysis Summary:\")\n",
    "print(f\"   üìù Text Summaries Generated: {len(text_summaries)}\")\n",
    "print(f\"   üìä Table Summaries Generated: {len(table_summaries)}\")\n",
    "print(f\"   üñºÔ∏è Image Summaries Generated: {len(image_summaries)}\")\n",
    "print()\n",
    "\n",
    "# Calculate success metrics\n",
    "total_content = len(text_summaries) + len(table_summaries) + len(image_summaries)\n",
    "print(f\"üéØ System Capabilities:\")\n",
    "print(f\"   üóÉÔ∏è Total Content Pieces Indexed: {total_content}\")\n",
    "print(f\"   üîç Multimodal Search: ‚úÖ Enabled\")\n",
    "print(f\"   ü§ñ AI-Powered Summarization: ‚úÖ Active\")\n",
    "print(f\"   üíæ Vector Database: ‚úÖ Populated\")\n",
    "print(f\"   ‚ùì Question Answering: ‚úÖ Ready\")\n",
    "\n",
    "print(f\"\\nüåü Key Achievements:\")\n",
    "print(f\"   ‚úÖ Successfully extracted financial data from complex PDF\")\n",
    "print(f\"   ‚úÖ Processed multiple content types (text, tables, images)\")\n",
    "print(f\"   ‚úÖ Created intelligent summaries of all content\")\n",
    "print(f\"   ‚úÖ Built searchable knowledge base\")\n",
    "print(f\"   ‚úÖ Enabled natural language querying\")\n",
    "\n",
    "print(f\"\\nüí° What makes this system special:\")\n",
    "print(f\"   üéØ Multimodal: Understands text, tables, AND images\")\n",
    "print(f\"   üß† Semantic Search: Finds relevant info by meaning, not just keywords\")\n",
    "print(f\"   üìä Financial Intelligence: Specialized for business document analysis\")\n",
    "print(f\"   üîÑ Adaptive: Falls back to manual extraction when needed\")\n",
    "print(f\"   üé™ End-to-End: Complete pipeline from PDF to answers\")\n",
    "\n",
    "print(\"\\nüéâ Your multimodal RAG system is fully operational!\")\n",
    "print(\"üöÄ Ready for real-world financial document analysis!\")\n",
    "\n",
    "# ============================================================================\n",
    "# END OF NOTEBOOK\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49475b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
